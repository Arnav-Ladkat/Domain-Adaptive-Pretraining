{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXYbKxuQrF25"
      },
      "source": [
        "This Repository Contains Code for Downloading Datasets we used in our Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I97EJBZWpaUZ"
      },
      "source": [
        "# Downloading the CS Dataset from S2ORC\n",
        "_We Do not Own this Dataset_\n",
        "\n",
        "The Dataset is from [S2ORC Project](https://github.com/allenai/s2orc) maintained by Allen AI.\n",
        "<br/>\n",
        "Please refer to their Repository for Access and Liscencing for use.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYG-1w_J5woO",
        "outputId": "76eb123a-59ec-4ed0-ec37-4aead6cb0fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCf1DS_e9eYX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Example of how one would download & process a single batch of S2ORC to filter to specific field of study.\n",
        "Can be useful for those who can't store the full dataset onto disk easily.\n",
        "Please adapt this to your own field of study.\n",
        "\n",
        "\n",
        "Creates directory structure:\n",
        "\n",
        "|-- metadata/\n",
        "    |-- raw/\n",
        "        |-- metadata_0.jsonl.gz      << input; deleted after processed\n",
        "    |-- medicine/\n",
        "        |-- metadata_0.jsonl         << output\n",
        "|-- pdf_parses/\n",
        "    |-- raw/\n",
        "        |-- pdf_parses_0.jsonl.gz    << input; deleted after processed\n",
        "    |-- medicine/\n",
        "        |-- pdf_parses_0.jsonl       << output\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import gzip\n",
        "import io\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# process single batch\n",
        "def process_batch(batch: dict):\n",
        "    # this downloads both the metadata & full text files for a particular shard\n",
        "    cmd = [\"wget\", \"-O\", batch['input_metadata_path'], batch['input_metadata_url']]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)\n",
        "\n",
        "    cmd = [\"wget\", \"-O\", batch['input_pdf_parses_path'], batch['input_pdf_parses_url']]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)\n",
        "\n",
        "    # first, let's filter metadata JSONL to only papers with a particular field of study.\n",
        "    # we also want to remember which paper IDs to keep, so that we can get their full text later.\n",
        "    paper_ids_to_keep = set()\n",
        "    with gzip.open(batch['input_metadata_path'], 'rb') as gz, open(batch['output_metadata_path'], 'wb') as f_out:\n",
        "        f = io.BufferedReader(gz)\n",
        "        for line in tqdm(f.readlines()):\n",
        "            metadata_dict = json.loads(line)\n",
        "            paper_id = metadata_dict['paper_id']\n",
        "            mag_field_of_study = metadata_dict['mag_field_of_study']\n",
        "            if mag_field_of_study and 'Computer Science' in mag_field_of_study:     # TODO: <<< change this to your filter\n",
        "                paper_ids_to_keep.add(paper_id)\n",
        "                f_out.write(line)\n",
        "\n",
        "    # now, we get those papers' full text\n",
        "    with gzip.open(batch['input_pdf_parses_path'], 'rb') as gz, open(batch['output_pdf_parses_path'], 'wb') as f_out:\n",
        "        f = io.BufferedReader(gz)\n",
        "        for line in tqdm(f.readlines()):\n",
        "            metadata_dict = json.loads(line)\n",
        "            paper_id = metadata_dict['paper_id']\n",
        "            if paper_id in paper_ids_to_keep:\n",
        "                f_out.write(line)\n",
        "\n",
        "    # now delete the raw files to clear up space for other shards\n",
        "    os.remove(batch['input_metadata_path'])\n",
        "    os.remove(batch['input_pdf_parses_path'])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    METADATA_INPUT_DIR = 'metadata/raw/'\n",
        "    METADATA_OUTPUT_DIR = '/content/drive/MyDrive/CsDataset/metadata/computer science/'\n",
        "    PDF_PARSES_INPUT_DIR = 'pdf_parses/raw/'\n",
        "    PDF_PARSES_OUTPUT_DIR = '/content/drive/MyDrive/CsDataset/pdf_parses/computer science/'\n",
        "\n",
        "    os.makedirs(METADATA_INPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(METADATA_OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(PDF_PARSES_INPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(PDF_PARSES_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # TODO: make sure to put the links we sent to you here\n",
        "    # there are 100 shards with IDs 0 to 99. make sure these are paired correctly.\n",
        "    download_linkss = [\n",
        "  {\n",
        "    \"metadata\": \"Add Shard Link here \",\n",
        "    \"pdf_parses\": \"Add Shard Link here \"\n",
        "  },\n",
        "  {\n",
        "    \"metadata\": \"Add Shard Link here \",\n",
        "    \"pdf_parses\": \"Add Shard Link here \"\n",
        "  },\n",
        "]\n",
        "\n",
        "    # turn these into batches of work\n",
        "    # TODO: feel free to come up with your own naming convention for 'input_{metadata|pdf_parses}_path'\n",
        "    batches = [{\n",
        "        'input_metadata_url': download_links['metadata'],\n",
        "        'input_metadata_path': os.path.join(METADATA_INPUT_DIR,\n",
        "                                            os.path.basename(download_links['metadata'].split('?')[0])),\n",
        "        'output_metadata_path': os.path.join(METADATA_OUTPUT_DIR,\n",
        "                                             os.path.basename(download_links['metadata'].split('?')[0])),\n",
        "        'input_pdf_parses_url': download_links['pdf_parses'],\n",
        "        'input_pdf_parses_path': os.path.join(PDF_PARSES_INPUT_DIR,\n",
        "                                              os.path.basename(download_links['pdf_parses'].split('?')[0])),\n",
        "        'output_pdf_parses_path': os.path.join(PDF_PARSES_OUTPUT_DIR,\n",
        "                                               os.path.basename(download_links['pdf_parses'].split('?')[0])),\n",
        "    } for download_links in download_linkss]\n",
        "\n",
        "    for batch in batches:\n",
        "        process_batch(batch=batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaVrU-c-q1GQ"
      },
      "source": [
        "# Downloading the Amazon Review Dataset (2018)\n",
        "\n",
        "_We Do not Own this Dataset_\n",
        "\n",
        "This Dataset is an updated version of the [Amazon review dataset](http://jmcauley.ucsd.edu/data/amazon/index_2014.html) released in 2014\n",
        "\n",
        "\n",
        "**We use the 5 Core version of the Dataset** \n",
        "\n",
        "The Dataset is created by [Jianmo Ni, UCSD](https://nijianmo.github.io/amazon/index.html#subsets) \n",
        "\n",
        "Please cite the following paper if you use the data in any way:\n",
        "\n",
        "Justifying recommendations using distantly-labeled reviews and fined-grained aspects\n",
        "Jianmo Ni, Jiacheng Li, Julian McAuley\n",
        "Empirical Methods in Natural Language Processing (EMNLP), 2019\n",
        "\n",
        "\n",
        "<br/>\n",
        "Please refer to their Repository for Access and Liscencing for use.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckgQKjDrt-hJ"
      },
      "source": [
        "List of URLs can be Found [here](https://nijianmo.github.io/amazon/index.html) \n",
        "\n",
        "We have created a simplified file for the same [here]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZsuQqqbs_uy"
      },
      "outputs": [],
      "source": [
        "with open('amazon5corelinks.txt', 'r') as f:\n",
        "    mainlist = [line for line in f]\n",
        "\n",
        "listofURLs= mainlist[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfLiHG9KmGfu"
      },
      "outputs": [],
      "source": [
        "listofPaths=[]\n",
        "for i in listofURLs:\n",
        "  listofPaths.append(i.replace(\"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/\",\"/content/drive/MyDrive/AmazonReview/\" ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WhGcDO2mXY-"
      },
      "outputs": [],
      "source": [
        "for x in range(len(listofURLs)-1):\n",
        "  print(x)\n",
        "  cmd = [\"wget\", l[x], \"-O\", k[x]]\n",
        "  subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOIrXlNlnHzL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "DownloadingDatasetsforDomainAdaptation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
